#!/usr/bin/env python3
"""
ç¶œåˆæ¸¬è©¦è…³æœ¬ - æ¸¬è©¦ LLM Prompt Injection Testing Platform çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
Comprehensive Test Script for LLM Prompt Injection Testing Platform
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Dict, List, Any
import json
import tempfile
import shutil

# æ·»åŠ  src åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from src.core.platform import PromptInjectionPlatform
    from src.providers.github_provider import GitHubProvider
    from src.providers.openai_provider import OpenAIProvider
    from src.core.basic_injection import BasicInjectionAttack
    from src.core.role_playing import RolePlayingAttack
    from src.core.reporter import ReportGenerator
    from src.core.evaluator import AttackEvaluator
except ImportError as e:
    print(f"âŒ å°å…¥éŒ¯èª¤: {e}")
    print("è«‹ç¢ºä¿åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")
    sys.exit(1)


class ComprehensiveTestSuite:
    """ç¶œåˆæ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        self.test_results = {}
        self.platform = None
        self.test_count = 0
        self.passed_tests = 0
        self.failed_tests = 0
        
    def log_test(self, test_name: str, success: bool, message: str = ""):
        """è¨˜éŒ„æ¸¬è©¦çµæœ"""
        self.test_count += 1
        if success:
            self.passed_tests += 1
            status = "âœ… PASS"
        else:
            self.failed_tests += 1
            status = "âŒ FAIL"
            
        print(f"{status} - {test_name}")
        if message:
            print(f"      {message}")
            
        self.test_results[test_name] = {
            "success": success,
            "message": message,
            "timestamp": time.time()
        }
    
    async def test_platform_initialization(self):
        """æ¸¬è©¦å¹³å°åˆå§‹åŒ–"""
        try:
            self.platform = PromptInjectionPlatform()
            self.log_test("å¹³å°åˆå§‹åŒ–", True, "Platform successfully initialized")
        except Exception as e:
            self.log_test("å¹³å°åˆå§‹åŒ–", False, f"Error: {e}")
            return False
        return True
    
    async def test_provider_creation(self):
        """æ¸¬è©¦æä¾›å•†å‰µå»º"""
        try:
            # æ¸¬è©¦ GitHub Provider
            github_provider = GitHubProvider("test-token")
            self.log_test("GitHub Provider å‰µå»º", True, f"Models: {github_provider.supported_models}")
            
            # æ¸¬è©¦ OpenAI Provider (å¦‚æœæœ‰ API key)
            import os
            if os.getenv("OPENAI_API_KEY"):
                openai_provider = OpenAIProvider()
                self.log_test("OpenAI Provider å‰µå»º", True, "OpenAI provider created successfully")
            else:
                self.log_test("OpenAI Provider å‰µå»º", True, "Skipped - no API key provided")
                
        except Exception as e:
            self.log_test("Provider å‰µå»º", False, f"Error: {e}")
            return False
        return True
    
    async def test_attack_modules(self):
        """æ¸¬è©¦æ”»æ“Šæ¨¡çµ„"""
        try:
            # æ¸¬è©¦åŸºæœ¬æ³¨å…¥æ”»æ“Š
            basic_attack = BasicInjectionAttack()
            payloads = basic_attack.get_payloads()
            self.log_test("åŸºæœ¬æ³¨å…¥æ”»æ“Šæ¨¡çµ„", len(payloads) > 0, f"Generated {len(payloads)} payloads")
            
            # æ¸¬è©¦è§’è‰²æ‰®æ¼”æ”»æ“Š
            role_attack = RolePlayingAttack()
            role_payloads = role_attack.get_payloads()
            self.log_test("è§’è‰²æ‰®æ¼”æ”»æ“Šæ¨¡çµ„", len(role_payloads) > 0, f"Generated {len(role_payloads)} role payloads")
            
        except Exception as e:
            self.log_test("æ”»æ“Šæ¨¡çµ„æ¸¬è©¦", False, f"Error: {e}")
            return False
        return True
    
    async def test_report_generation(self):
        """æ¸¬è©¦å ±å‘Šç”Ÿæˆ"""
        try:
            # å‰µå»ºè‡¨æ™‚è¼¸å‡ºç›®éŒ„
            with tempfile.TemporaryDirectory() as temp_dir:
                reporter = ReportGenerator(temp_dir)
                
                # å‰µå»ºæ¨¡æ“¬æ¸¬è©¦çµæœ
                mock_results = []
                mock_evaluation = type('MockEval', (), {
                    'total_attacks': 5,
                    'successful_attacks': 2,
                    'success_rate': 0.4,
                    'average_confidence': 0.75,
                    'risk_distribution': {'high': 1, 'medium': 1, 'low': 3},
                    'attack_type_breakdown': {
                        'basic_injection': {'successful': 1, 'total': 3, 'success_rate': 0.33},
                        'role_playing': {'successful': 1, 'total': 2, 'success_rate': 0.5}
                    },
                    'provider_analysis': {},
                    'timestamp': '2024-01-01T00:00:00',
                    'metadata': {}
                })()
                
                # ç”Ÿæˆå ±å‘Š
                report_files = reporter.generate_comprehensive_report(
                    mock_results, 
                    mock_evaluation, 
                    formats=["markdown", "json"]
                )
                
                # æª¢æŸ¥å ±å‘Šæª”æ¡ˆæ˜¯å¦ç”Ÿæˆ
                markdown_exists = Path(report_files.get("markdown", "")).exists()
                json_exists = Path(report_files.get("json", "")).exists()
                
                self.log_test("Markdown å ±å‘Šç”Ÿæˆ", markdown_exists, "Markdown report generated")
                self.log_test("JSON å ±å‘Šç”Ÿæˆ", json_exists, "JSON report generated")
                
        except Exception as e:
            self.log_test("å ±å‘Šç”Ÿæˆæ¸¬è©¦", False, f"Error: {e}")
            return False
        return True
    
    async def test_evaluator(self):
        """æ¸¬è©¦è©•ä¼°å™¨"""
        try:
            evaluator = AttackEvaluator()
            
            # æ¨¡æ“¬æ”»æ“Šçµæœ
            mock_response = "I cannot help with that request as it violates my guidelines."
            result = evaluator.evaluate_response(mock_response, {"expected": "should_reject"})
            
            self.log_test("è©•ä¼°å™¨åŠŸèƒ½", result is not None, f"Evaluation result: {result}")
            
        except Exception as e:
            self.log_test("è©•ä¼°å™¨æ¸¬è©¦", False, f"Error: {e}")
            return False
        return True
    
    async def test_configuration_loading(self):
        """æ¸¬è©¦é…ç½®æ–‡ä»¶åŠ è¼‰"""
        try:
            # æª¢æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            config_files = [
                "configs/defaults.yaml",
                "configs/providers.yaml", 
                "configs/custom_attacks.yaml"
            ]
            
            for config_file in config_files:
                if Path(config_file).exists():
                    self.log_test(f"{config_file} å­˜åœ¨æ€§æª¢æŸ¥", True, f"Config file found: {config_file}")
                else:
                    self.log_test(f"{config_file} å­˜åœ¨æ€§æª¢æŸ¥", False, f"Config file missing: {config_file}")
                    
            # æ¸¬è©¦ YAML åŠ è¼‰
            import yaml
            for config_file in config_files:
                if Path(config_file).exists():
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            yaml.safe_load(f)
                        self.log_test(f"{config_file} YAML èªæ³•", True, "Valid YAML syntax")
                    except yaml.YAMLError as e:
                        self.log_test(f"{config_file} YAML èªæ³•", False, f"Invalid YAML: {e}")
                        
        except Exception as e:
            self.log_test("é…ç½®æ–‡ä»¶æ¸¬è©¦", False, f"Error: {e}")
            return False
        return True
    
    async def test_template_system(self):
        """æ¸¬è©¦æ¨¡æ¿ç³»çµ±"""
        try:
            template_file = Path("src/templates/prompt_injection_template.md")
            
            if template_file.exists():
                # è®€å–æ¨¡æ¿å…§å®¹
                template_content = template_file.read_text(encoding='utf-8')
                
                # æª¢æŸ¥æ¨¡æ¿æ˜¯å¦åŒ…å«å¿…è¦çš„ä½”ä½ç¬¦
                required_placeholders = [
                    "{{generated_at}}",
                    "{{total_attacks}}",
                    "{{success_rate}}",
                    "{{risk_level}}"
                ]
                
                missing_placeholders = []
                for placeholder in required_placeholders:
                    if placeholder not in template_content:
                        missing_placeholders.append(placeholder)
                
                if not missing_placeholders:
                    self.log_test("æ¨¡æ¿ç³»çµ±", True, f"Template contains all required placeholders")
                else:
                    self.log_test("æ¨¡æ¿ç³»çµ±", False, f"Missing placeholders: {missing_placeholders}")
            else:
                self.log_test("æ¨¡æ¿ç³»çµ±", False, "Template file not found")
                
        except Exception as e:
            self.log_test("æ¨¡æ¿ç³»çµ±æ¸¬è©¦", False, f"Error: {e}")
            return False
        return True
    
    async def test_environment_variables(self):
        """æ¸¬è©¦ç’°å¢ƒè®Šæ•¸"""
        import os
        
        # æª¢æŸ¥ .env.example æ˜¯å¦å­˜åœ¨
        env_example = Path(".env.example")
        if env_example.exists():
            self.log_test(".env.example å­˜åœ¨", True, "Environment example file found")
        else:
            self.log_test(".env.example å­˜åœ¨", False, "Environment example file missing")
        
        # æª¢æŸ¥é—œéµç’°å¢ƒè®Šæ•¸
        key_vars = ["GITHUB_TOKEN", "OPENAI_API_KEY"]
        for var in key_vars:
            value = os.getenv(var)
            if value:
                self.log_test(f"ç’°å¢ƒè®Šæ•¸ {var}", True, f"Set (length: {len(value)})")
            else:
                self.log_test(f"ç’°å¢ƒè®Šæ•¸ {var}", True, f"Not set (optional)")
    
    async def test_script_executability(self):
        """æ¸¬è©¦è…³æœ¬å¯åŸ·è¡Œæ€§"""
        scripts = [
            "scripts/run_platform.sh",
            "scripts/test_functionality.sh",
            "scripts/validate_configs.sh"
        ]
        
        for script in scripts:
            script_path = Path(script)
            if script_path.exists():
                # æª¢æŸ¥æ˜¯å¦å¯åŸ·è¡Œ
                import stat
                if script_path.stat().st_mode & stat.S_IEXEC:
                    self.log_test(f"{script} å¯åŸ·è¡Œæ€§", True, "Script is executable")
                else:
                    self.log_test(f"{script} å¯åŸ·è¡Œæ€§", False, "Script is not executable")
            else:
                self.log_test(f"{script} å­˜åœ¨æ€§", False, "Script file missing")
    
    async def test_docker_configuration(self):
        """æ¸¬è©¦ Docker é…ç½®"""
        docker_files = ["Dockerfile", "docker-compose.yml"]
        
        for docker_file in docker_files:
            if Path(docker_file).exists():
                self.log_test(f"{docker_file} å­˜åœ¨", True, f"Docker config found: {docker_file}")
            else:
                self.log_test(f"{docker_file} å­˜åœ¨", False, f"Docker config missing: {docker_file}")
    
    async def test_output_directory_creation(self):
        """æ¸¬è©¦è¼¸å‡ºç›®éŒ„å‰µå»º"""
        try:
            # æ¸¬è©¦å‰µå»ºè¼¸å‡ºç›®éŒ„
            test_output_dir = Path("test_output_temp")
            test_output_dir.mkdir(exist_ok=True)
            
            # å‰µå»ºå­ç›®éŒ„
            (test_output_dir / "reports").mkdir(exist_ok=True)
            (test_output_dir / "data").mkdir(exist_ok=True)
            
            self.log_test("è¼¸å‡ºç›®éŒ„å‰µå»º", True, "Output directories created successfully")
            
            # æ¸…ç†æ¸¬è©¦ç›®éŒ„
            shutil.rmtree(test_output_dir)
            
        except Exception as e:
            self.log_test("è¼¸å‡ºç›®éŒ„å‰µå»º", False, f"Error: {e}")
            return False
        return True
    
    async def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("ğŸ§ª é–‹å§‹åŸ·è¡Œç¶œåˆåŠŸèƒ½æ¸¬è©¦")
        print("=" * 60)
        
        # åŸ·è¡Œå„é …æ¸¬è©¦
        test_functions = [
            self.test_platform_initialization,
            self.test_provider_creation,
            self.test_attack_modules,
            self.test_report_generation,
            self.test_evaluator,
            self.test_configuration_loading,
            self.test_template_system,
            self.test_environment_variables,
            self.test_script_executability,
            self.test_docker_configuration,
            self.test_output_directory_creation
        ]
        
        for test_func in test_functions:
            try:
                await test_func()
            except Exception as e:
                self.log_test(test_func.__name__, False, f"Unexpected error: {e}")
        
        # è¼¸å‡ºæ¸¬è©¦çµæœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
        print(f"ç¸½æ¸¬è©¦æ•¸: {self.test_count}")
        print(f"é€šé: {self.passed_tests}")
        print(f"å¤±æ•—: {self.failed_tests}")
        print(f"æˆåŠŸç‡: {(self.passed_tests/self.test_count*100):.1f}%")
        
        if self.failed_tests == 0:
            print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¹³å°æº–å‚™å°±ç·’ã€‚")
            return True
        else:
            print(f"\nâš ï¸  {self.failed_tests} å€‹æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤è¨Šæ¯ã€‚")
            return False


async def main():
    """ä¸»å‡½æ•¸"""
    test_suite = ComprehensiveTestSuite()
    success = await test_suite.run_all_tests()
    
    # ä¿å­˜æ¸¬è©¦çµæœ
    with open("test_results.json", "w", encoding="utf-8") as f:
        json.dump(test_suite.test_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ è©³ç´°æ¸¬è©¦çµæœå·²ä¿å­˜è‡³ test_results.json")
    
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)