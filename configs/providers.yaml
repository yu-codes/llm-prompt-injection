# Provider Configuration Examples
# This file shows how to configure different LLM providers

providers:
  # GitHub Models Configuration (Default)
  github:
    enabled: true
    api_key: "${GITHUB_TOKEN}"  # GitHub Personal Access Token
    base_url: "https://models.inference.ai.azure.com"
    models:
      - "gpt-4o"
      - "gpt-4o-mini"
      - "o1-preview"
      - "o1-mini"
    default_model: "gpt-4o-mini"
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    retry_attempts: 3
    retry_delay: 1.0
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 100000

  # OpenAI Configuration
  openai:
    enabled: false
    api_key: "${OPENAI_API_KEY}"  # From environment variable
    base_url: "https://api.openai.com/v1"
    model: "gpt-3.5-turbo"
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
    retry_attempts: 3
    retry_delay: 1
    rate_limit:
      requests_per_minute: 20
      tokens_per_minute: 40000

  # Example: OpenAI with different model
  openai_gpt4:
    enabled: false
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model: "gpt-4"
    temperature: 0.5
    max_tokens: 2000
    timeout: 60
    retry_attempts: 2
    retry_delay: 2

  # Example: Azure OpenAI (placeholder)
  azure_openai:
    enabled: false
    api_key: "${AZURE_OPENAI_API_KEY}"
    base_url: "${AZURE_OPENAI_ENDPOINT}"
    api_version: "2023-05-15"
    deployment_name: "${AZURE_DEPLOYMENT_NAME}"
    model: "gpt-35-turbo"
    temperature: 0.7
    max_tokens: 1000

  # Example: Local LLM (placeholder)
  local_llm:
    enabled: false
    base_url: "http://localhost:8000/v1"
    model: "local-model"
    api_key: "not-required"
    temperature: 0.8
    max_tokens: 1500
    timeout: 120

# Default provider (used when none specified)
default_provider: "github"

# Global settings
global:
  concurrent_requests: 3  # Max concurrent requests across all providers
  request_timeout: 30
  log_level: "INFO"
  enable_detailed_logging: false